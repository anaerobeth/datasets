---
annotations_creators:
- crowdsourced
language_creators:
- found
languages:
- en
licenses: []
multilinguality:
- monolingual
size_categories:
- n<1K
source_datasets:
- extended|other-trivia-qa
task_categories:
- question-answering
task_ids:
- open-domain-qa
---

# Dataset Card for FreebaseQA

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)

## Dataset Description

- **Homepage:**
- **Repository: [FreebaseQA repository](https://github.com/kelvin-jiang/FreebaseQA)**
- **Paper: [FreebaseQA paper](https://www.aclweb.org/anthology/N19-1028.pdf)**
- **Leaderboard:**
- **Point of Contact: [Kelvin Jiang](https://github.com/kelvin-jiang)**

### Dataset Summary

FreebaseQA is a dataset for open-domain factoid question answering (QA) tasks over structured knowledge bases, like Freebase.

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

English

## Dataset Structure

### Data Instances

```
{
  "Questions": [
    {
      "Question-ID": "FreebaseQA-eval-0", 
      "RawQuestion": "Who is the female presenter of the Channel 4 quiz show '1001 things you should know'?", 
      "ProcessedQuestion": "who is the female presenter of the channel 4 quiz show '1001 things you should know'", 
      "Parses": [
        {
          "Parse-Id": "FreebaseQA-eval-0.P0", 
          "PotentialTopicEntityMention": "1001 things you should know", 
          "TopicEntityName": "1001 things you should know", 
          "TopicEntityMid": "m.0nd3t34", 
          "InferentialChain": "tv.tv_program.regular_personal_appearances..tv.tv_regular_personal_appearance.person", 
          "Answers": [
            {
              "AnswersMid": "m.0216y_", 
              "AnswersName": [
                "sandi toksvig"
              ]
            }
          ]
        }
      ]
    }, 
   ...
  ]
}
```

### Data Fields
Questions: The set of unique questions in this data set
Question-ID: The unique ID of each question
RawQuestion: The original question collected from data sources
ProcessedQuestion: The question processed with some operations such as removal of trailing question mark and decapitalization
Parses: The semantic parse(s) for the question
Parse-Id: The ID of each semantic parse
PotentialTopicEntityMention: The potential topic entity mention in the question
TopicEntityName: The name or alias of the topic entity in the question from Freebase
TopicEntityMid: The Freebase MID of the topic entity in the question
InferentialChain: The path from the topic entity node to the answer node in Freebase, labeled as a predicate
Answers: The answer found from this parse
AnswersMid: The Freebase MID of the answer
AnswersName: The answer string from the original question-answer pair

### Data Splits
This data set contains 28,348 unique questions that are divided into three subsets: train (20,358), dev (3,994) and eval (3,996), formatted as JSON files: FreebaseQA-[train|dev|eval].json

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

#### Initial Data Collection and Normalization

The data set is generated by matching trivia-type question-answer pairs with subject-predicateobject triples in Freebase. For each collected question-answer pair, we first tag all entities in each question and search for relevant predicates that bridge a tagged entity with the answer in Freebase. Finally, human annotation is used to remove false positives in these matched triples. 

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

Kelvin Jiang - Currently at University of Waterloo. Work was done at
York University

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]
